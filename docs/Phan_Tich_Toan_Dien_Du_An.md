# PH√ÇN T√çCH TO√ÄN DI·ªÜN D·ª∞ √ÅN MACHINE LEARNING
## "D·ª∞ ƒêO√ÅN ƒê·ªò TH√ÄNH C√îNG C·ª¶A PHIM"

*Ng√†y ph√¢n t√≠ch: 6 th√°ng 11, 2025*

---

## I. TI·∫æN ƒê·ªò THEO TU·∫¶N

### Tu·∫ßn 1-3: Thu th·∫≠p, L√†m s·∫°ch v√† T·∫°o nh√£n d·ªØ li·ªáu

#### **1. Load v√† L√†m s·∫°ch d·ªØ li·ªáu (Tu·∫ßn 1-2)**
- **File th·ª±c hi·ªán:** 
  - D·ªØ li·ªáu ban ƒë·∫ßu: `data/raw_Movies.csv`
  - Script l√†m s·∫°ch: `progress/week02/cleandata.py`
  
- **X·ª≠ l√Ω d·ªØ li·ªáu:**
  - **Missing/Invalid Values:** Lo·∫°i b·ªè c√°c phim c√≥ `Budget` ho·∫∑c `Revenue` b·∫±ng 0
  - **K·∫øt qu·∫£:** Gi·∫£m t·ª´ 2,194 phim xu·ªëng 1,020 phim (lo·∫°i b·ªè 1,173 phim)
  - **ƒê·ªãnh d·∫°ng:** Chu·∫©n h√≥a c·ªôt `Release Date`
  
- **Output:** D·ªØ li·ªáu s·∫°ch ƒë∆∞·ª£c l∆∞u t·∫°i `data/clean_movies.csv`

#### **2. T·∫°o bi·∫øn m·ª•c ti√™u (Tu·∫ßn 3)**
- **File th·ª±c hi·ªán:** `progress/week03/crea_label.ipynb`

- **ƒê·ªãnh nghƒ©a th√†nh c√¥ng:** M·ªôt phim ƒë∆∞·ª£c coi l√† th√†nh c√¥ng (`success = 1`) khi th·ªèa m√£n ƒë·ªìng th·ªùi:
  - `ROI >= 1.0` (v·ªõi `ROI = Revenue / Budget`)
  - `Vote Average >= 6.5`

- **K·∫øt qu·∫£ ph√¢n ph·ªëi:**
  - 514 phim th√†nh c√¥ng (50.4%)
  - 506 phim th·∫•t b·∫°i (49.6%)
  - D·ªØ li·ªáu kh√° c√¢n b·∫±ng

- **Output:** `data/clean_movies_with_labels.csv`

### Tu·∫ßn 4: Feature Engineering

#### **File th·ª±c hi·ªán:** `progress/week04/feature_engineering.ipynb`

#### **C√°c b∆∞·ªõc ch√≠nh:**

**1. One-Hot Encoding:**
- √Åp d·ª•ng cho c·ªôt `genres` v√† `production_countries`
- T·∫°o ra c√°c c·ªôt vector: `genre_Action`, `genre_Comedy`, `is_usa`

**2. Feature Engineering:**

**T√†i ch√≠nh:**
- `roi` (Return on Investment)
- `roi_clipped` (ROI gi·ªõi h·∫°n)
- `Budget_log`, `Revenue_log`

**Th·ªùi gian:**
- `release_year`, `release_month`, `release_quarter`
- `is_holiday_season`

**N·ªôi dung:**
- `num_genres`, `num_main_cast`

**T∆∞∆°ng t√°c:**
- `roi_vs_vote` (k·∫øt h·ª£p ROI v√† ƒëi·ªÉm ƒë√°nh gi√°)

**3. K·∫øt qu·∫£:** B·ªô d·ªØ li·ªáu v·ªõi 65 ƒë·∫∑c tr∆∞ng ‚Üí `data/clean_movies_features.csv`

### Tu·∫ßn 5: Modeling v√† ƒê√°nh gi√°

#### **1. Chu·∫©n b·ªã d·ªØ li·ªáu**
- **File:** `progress/week05/data_split.py`
- **Quy tr√¨nh:**
  - Ch·ªçn 47 features cu·ªëi c√πng
  - Chia train/test (80%/20%) v·ªõi stratified sampling
  - √Åp d·ª•ng `MinMaxScaler` [0, 1]
  - X·ª≠ l√Ω missing values: `fillna(0)`
  - **Kh√¥ng c·∫ßn SMOTE** (d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng)

#### **2. M√¥ h√¨nh Baseline (Logistic Regression)**
- **File:** `progress/week05/Logistic_Regression_Model/logistic_regression.py`
- **Tham s·ªë:** `max_iter=1000`, `random_state=42`
- **K·∫øt qu·∫£:**
  - Accuracy: 84.80%
  - F1-Score: 84.88%

#### **3. M√¥ h√¨nh n√¢ng cao (Random Forest)**
- **File:** `progress/week05/Random_Forest_Model/random_forest.py`
- **Tham s·ªë:** `random_state=42` (m·∫∑c ƒë·ªãnh)
- **K·∫øt qu·∫£:**
  - Accuracy: 99.51%
  - F1-Score: 99.52%
  - Recall: 100.00%
  - Precision: 99.04%
- **Cross-Validation:** F1-Score trung b√¨nh `99.88% ¬± 0.14%`

#### **4. Feature Importance Analysis**
- **File:** `progress/week05/phan_tich_dac_trung/feature_importance.py`
- **K·∫øt qu·∫£ ph√¢n t√≠ch:** `Vote Average` l√† y·∫øu t·ªë quan tr·ªçng nh·∫•t

---

## II. K·∫æT QU·∫¢ QUAN TR·ªåNG

### 1. M√¥ h√¨nh t·ªët nh·∫•t hi·ªán t·∫°i
**Random Forest** ƒë∆∞·ª£c ch·ªçn l√†m m√¥ h√¨nh cu·ªëi c√πng do hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi.

### 2. Ch·ªâ s·ªë quan tr·ªçng nh·∫•t (Random Forest)

| Metric | Gi√° tr·ªã |
|--------|---------|
| **Accuracy** | 99.51% |
| **F1-Score** | 99.52% |
| **Recall** | 100.00% |
| **Precision** | 99.04% |
| **CV F1-Score** | 99.88% ¬± 0.14% |

### 3. Top 5 Feature quan tr·ªçng nh·∫•t

| Rank | Feature | Importance |
|------|---------|------------|
| 1 | `Vote Average` | 41.56% |
| 2 | `roi_vs_vote` | 10.70% |
| 3 | `roi` | 7.05% |
| 4 | `roi_clipped` | 6.27% |
| 5 | `Vote Count` | 5.14% |

### 4. Insight ch√≠nh t·ª´ d·ªØ li·ªáu

#### **Feature m·∫°nh nh·∫•t:** 
`Vote Average` l√† y·∫øu t·ªë quy·∫øt ƒë·ªãnh, cho th·∫•y **ch·∫•t l∆∞·ª£ng n·ªôi dung** quan tr·ªçng h∆°n ng√¢n s√°ch.

#### **Xu h∆∞·ªõng quan tr·ªçng:**
- Features li√™n quan ƒë·∫øn hi·ªáu qu·∫£ t√†i ch√≠nh (`roi`, `roi_vs_vote`) ƒë·ª©ng ngay sau `Vote Average`
- M√¥ h√¨nh h·ªçc ƒë∆∞·ª£c ƒë√∫ng ƒë·ªãnh nghƒ©a th√†nh c√¥ng (c·∫£ ch·∫•t l∆∞·ª£ng v√† t√†i ch√≠nh)

#### **C√¥ng th·ª©c th√†nh c√¥ng:**
Phim c√≥ kh·∫£ nƒÉng th√†nh c√¥ng cao nh·∫•t khi: **ƒê∆∞·ª£c kh√°n gi·∫£ ƒë√°nh gi√° cao** + **C√≥ kh·∫£ nƒÉng sinh l·ªùi t·ªët**

---

## III. PH√ÇN T√çCH THI·∫æU S√ìT / V·∫§N ƒê·ªÄ

### 1. T√≠nh m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu ‚úÖ
- **T√¨nh tr·∫°ng:** D·ªØ li·ªáu c√¢n b·∫±ng (50.4%/49.6%)
- **X·ª≠ l√Ω:** Kh√¥ng c·∫ßn SMOTE - quy·∫øt ƒë·ªãnh h·ª£p l√Ω

### 2. R·ªßi ro Overfitting ‚ö†Ô∏è
- **D·∫•u hi·ªáu:** Random Forest ƒë·∫°t hi·ªáu su·∫•t g·∫ßn ho√†n h·∫£o (99.51% Accuracy, 100% Recall)
- **Nguy√™n nh√¢n ti·ªÅm ·∫©n:** 
  - Feature `roi_vs_vote` c√≥ th·ªÉ "m·ªõm" c√¢u tr·∫£ l·ªùi
  - Ch∆∞a th·ª±c hi·ªán hyperparameter tuning ƒë·ªÉ ki·ªÉm so√°t overfitting

### 3. C√°c b∆∞·ªõc ch∆∞a t·ª± ƒë·ªông h√≥a ‚ö†Ô∏è
- **Pipeline r·ªùi r·∫°c:** Quy tr√¨nh chia th√†nh nhi·ªÅu script/notebook
- **Thi·∫øu tuning:** Random Forest s·ª≠ d·ª•ng tham s·ªë m·∫∑c ƒë·ªãnh
- **Ch∆∞a c√≥ end-to-end pipeline:** C·∫ßn t√≠ch h·ª£p th√†nh m·ªôt pipeline duy nh·∫•t

### 4. File ch∆∞a th·ªëng nh·∫•t ‚ö†Ô∏è
- **C·∫•u tr√∫c t·ªët:** T·ªï ch·ª©c theo tu·∫ßn r√µ r√†ng
- **Thi·∫øu:** Docstrings chu·∫©n cho c√°c h√†m
- **C·∫ßn c·∫£i thi·ªán:** T·ª± ƒë·ªông h√≥a quy tr√¨nh ch·∫°y tu·∫ßn t·ª±

---

## IV. L∆ØU √ù QUAN TR·ªåNG KHI VI·∫æT B√ÅO C√ÅO

### 1. Danh s√°ch c√¢u h·ªèi gi·∫£ng vi√™n c√≥ th·ªÉ h·ªèi

1. **"T·∫°i sao Random Forest ƒë·∫°t 99.5% accuracy? C√≥ overfitting kh√¥ng?"**
   - C·∫ßn gi·∫£i th√≠ch v·ªÅ cross-validation results
   - Th·∫£o lu·∫≠n v·ªÅ nguy c∆° overfitting v√† c√°ch ki·ªÉm ch·ª©ng

2. **"Feature `roi_vs_vote` ƒë∆∞·ª£c t·∫°o nh∆∞ th·∫ø n√†o? C√≥ data leakage kh√¥ng?"**
   - Gi·∫£i th√≠ch c√°ch t√≠nh to√°n feature n√†y
   - Th·∫£o lu·∫≠n v·ªÅ kh·∫£ nƒÉng "m·ªõm" th√¥ng tin cho m√¥ h√¨nh

3. **"ƒê·ªãnh nghƒ©a 'th√†nh c√¥ng' (ROI ‚â• 1 v√† Vote ‚â• 6.5) c√≥ h·ª£p l√Ω kh√¥ng?"**
   - C·∫ßn ch·ª©ng minh t√≠nh h·ª£p l√Ω c·ªßa ng∆∞·ª°ng
   - Th·ª≠ nghi·ªám v·ªõi ng∆∞·ª°ng kh√°c n·∫øu c√≥ th·ªÉ

4. **"M√¥ h√¨nh c√≥ Recall 100% - sai l·∫ßm duy nh·∫•t l√† g√¨?"**
   - Ph√¢n t√≠ch chi ti·∫øt False Positive case
   - Gi·∫£i th√≠ch t·∫°i sao m√¥ h√¨nh d·ª± ƒëo√°n sai

5. **"T·∫°i sao kh√¥ng th·ª±c hi·ªán Hyperparameter Tuning?"**
   - K·∫ø ho·∫°ch c·∫£i thi·ªán m√¥ h√¨nh
   - So s√°nh v·ªõi m√¥ h√¨nh ƒë√£ tuning

### 2. C√°c m·ª•c b·∫Øt bu·ªôc trong b√°o c√°o

#### **A. M√¥ t·∫£ Dataset**
- Ngu·ªìn g·ªëc v√† quy m√¥: 2,194 ‚Üí 1,020 phim
- ƒê·ªãnh nghƒ©a bi·∫øn m·ª•c ti√™u `success`
- Ph√¢n ph·ªëi classes v√† t√≠nh c√¢n b·∫±ng

#### **B. Ti·ªÅn x·ª≠ l√Ω v√† Feature Engineering**
- Lo·∫°i b·ªè gi√° tr·ªã 0 trong Budget/Revenue
- T·∫°o 47 features t·ª´ 65 features ban ƒë·∫ßu
- One-hot encoding cho categorical variables
- MinMaxScaler cho numerical features

#### **C. X√¢y d·ª±ng m√¥ h√¨nh**
- Logistic Regression (baseline)
- Random Forest (m√¥ h√¨nh ch√≠nh)
- Tham s·ªë v√† c·∫•u h√¨nh

#### **D. K·∫øt qu·∫£ v√† ƒê√°nh gi√°**
- B·∫£ng so s√°nh performance metrics
- Cross-validation results
- Confusion matrix analysis

#### **E. Feature Importance Analysis**
- Top 10 features quan tr·ªçng nh·∫•t
- Gi·∫£i th√≠ch √Ω nghƒ©a business c·ªßa t·ª´ng feature
- Bi·ªÉu ƒë·ªì visualization

#### **F. Ph√¢n t√≠ch l·ªói**
- False Positive/Negative analysis
- Business impact c·ªßa prediction errors
- √ù nghƒ©a th·ª±c ti·ªÖn

### 3. ƒêi·ªÉm nh·∫•n trong K·∫øt lu·∫≠n

#### **Insight th·ª±c t·∫ø:**
- **Ch·∫•t l∆∞·ª£ng n·ªôi dung (Vote Average) l√† y·∫øu t·ªë ti√™n quy·∫øt** cho th√†nh c√¥ng
- Quan tr·ªçng h∆°n c·∫£ ng√¢n s√°ch v√† marketing

#### **K·∫øt qu·∫£ m√¥ h√¨nh:**
- Random Forest: F1-Score 99.52%, CV stability cao
- Ti·ªÅm nƒÉng ·ª©ng d·ª•ng th·ª±c ti·ªÖn xu·∫•t s·∫Øc

#### **·ª®ng d·ª•ng th·ª±c ti·ªÖn:**
- H·ªó tr·ª£ nh√† s·∫£n xu·∫•t ƒë√°nh gi√° r·ªßi ro ƒë·∫ßu t∆∞
- T·∫≠p trung ngu·ªìn l·ª±c v√†o ch·∫•t l∆∞·ª£ng k·ªãch b·∫£n
- Tool decision-making cho ng√†nh ƒëi·ªán ·∫£nh

#### **H·∫°n ch·∫ø v√† H∆∞·ªõng ph√°t tri·ªÉn:**
- C·∫ßn ki·ªÉm tra overfitting k·ªπ h∆°n
- Hyperparameter tuning cho Random Forest
- Th·ª≠ nghi·ªám XGBoost, SVM
- X√¢y d·ª±ng end-to-end pipeline

---

## V. SUMMARY (PHONG C√ÅCH H·ªåC THU·∫¨T)

### üéØ **M·ª•c ti√™u**
X√¢y d·ª±ng m√¥ h√¨nh ML d·ª± ƒëo√°n th√†nh c√¥ng phim t·ª´ 1,020 phim ƒë√£ l√†m s·∫°ch.

### üî¨ **Ph∆∞∆°ng ph√°p**
- Feature Engineering: 47 ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu g·ªëc
- Baseline: Logistic Regression
- Main model: Random Forest
- Validation: 5-fold Cross-Validation

### üìä **ƒê·ªãnh nghƒ©a th√†nh c√¥ng**
`ROI ‚â• 1.0` AND `Vote Average ‚â• 6.5` ‚Üí D·ªØ li·ªáu c√¢n b·∫±ng (50.4% success)

### üèÜ **K·∫øt qu·∫£**
- **Random Forest F1-Score: 99.52%**
- **Recall: 100%** (kh√¥ng b·ªè l·ª° phim th√†nh c√¥ng)
- **CV Stability: 99.88% ¬± 0.14%**

### üîç **Ph√°t hi·ªán ch√≠nh**
**Vote Average = 41.56% importance** ‚Üí Ch·∫•t l∆∞·ª£ng n·ªôi dung l√† ch√¨a kh√≥a quy·∫øt ƒë·ªãnh

### ‚úÖ **K·∫øt lu·∫≠n**
M√¥ h√¨nh Random Forest c√≥ kh·∫£ nƒÉng d·ª± ƒëo√°n ch√≠nh x√°c cao, cung c·∫•p insight gi√° tr·ªã cho ng√†nh ƒëi·ªán ·∫£nh. C·∫ßn nghi√™n c·ª©u th√™m v·ªÅ overfitting v√† optimization.

---

*B√°o c√°o ƒë∆∞·ª£c t·∫°o b·ªüi: GitHub Copilot*
*D·ª± √°n: Machine Learning - D·ª± ƒëo√°n th√†nh c√¥ng phim*
*Repository: Do_An_1*